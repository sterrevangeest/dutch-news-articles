{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Categories from Named Entities in NOS Articles\n",
    "\n",
    "Final Assignment for Fudamentals of Machine Learning \n",
    "by Sterre van Geest\n",
    "\n",
    "January 2021\n",
    "\n",
    "\n",
    "## Introduction\n",
    "-----\n",
    "\n",
    "I am using the [Dutch News Articles](https://www.kaggle.com/maxscheijen/dutch-news-articles) dataset I found on Kaggle.com. The dataset contains all the articles published by the NOS as of the 1st of January 2010 till the 1st of January 2021. The data is obtained by scraping the NOS website. The NOS is one of the biggest (online) news organizations in the Netherlands. I first found it hard to give direction to the research. But I decided I want to research if I'm able to predict what category an article belongs to by analyzing named entities inside the articles. This may not have the most practical relevance but it was an interesting way for me to get started with Natural Language Processing in combination with named entities, which is something I am interested in.  \n",
    "\n",
    "\n",
    "## The dataset\n",
    "---\n",
    "\n",
    "\n",
    "### Getting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The dataset includes the following columns** (also shown below):\n",
    "\n",
    "- **datetime**: date and time of publication of the article.\n",
    "- **title**: the title of the news article.\n",
    "- **content**: the content of the news article.\n",
    "- **category**: the category under which the NOS filed the article.\n",
    "- **url**: link to the original article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-01-01 00:49:00</td>\n",
       "      <td>Enige Litouwse kerncentrale dicht</td>\n",
       "      <td>De enige kerncentrale van Litouwen is oudjaars...</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126231-enige-litouwse-k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-01-01 02:08:00</td>\n",
       "      <td>Spanje eerste EU-voorzitter onder nieuw verdrag</td>\n",
       "      <td>Spanje is met ingang van vandaag voorzitter va...</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126230-spanje-eerste-eu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010-01-01 02:09:00</td>\n",
       "      <td>Fout justitie in Blackwater-zaak</td>\n",
       "      <td>Vijf werknemers van het omstreden Amerikaanse ...</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126233-fout-justitie-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010-01-01 05:14:00</td>\n",
       "      <td>Museumplein vol, minder druk in Rotterdam</td>\n",
       "      <td>Het Oud en Nieuwfeest op het Museumplein in Am...</td>\n",
       "      <td>Binnenland</td>\n",
       "      <td>https://nos.nl/artikel/126232-museumplein-vol-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010-01-01 05:30:00</td>\n",
       "      <td>Obama krijgt rapporten over aanslag</td>\n",
       "      <td>President Obama heeft de eerste rapporten gekr...</td>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126236-obama-krijgt-rap...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime                                            title  \\\n",
       "0  2010-01-01 00:49:00                Enige Litouwse kerncentrale dicht   \n",
       "1  2010-01-01 02:08:00  Spanje eerste EU-voorzitter onder nieuw verdrag   \n",
       "2  2010-01-01 02:09:00                 Fout justitie in Blackwater-zaak   \n",
       "3  2010-01-01 05:14:00        Museumplein vol, minder druk in Rotterdam   \n",
       "4  2010-01-01 05:30:00              Obama krijgt rapporten over aanslag   \n",
       "\n",
       "                                             content    category  \\\n",
       "0  De enige kerncentrale van Litouwen is oudjaars...  Buitenland   \n",
       "1  Spanje is met ingang van vandaag voorzitter va...  Buitenland   \n",
       "2  Vijf werknemers van het omstreden Amerikaanse ...  Buitenland   \n",
       "3  Het Oud en Nieuwfeest op het Museumplein in Am...  Binnenland   \n",
       "4  President Obama heeft de eerste rapporten gekr...  Buitenland   \n",
       "\n",
       "                                                 url  \n",
       "0  https://nos.nl/artikel/126231-enige-litouwse-k...  \n",
       "1  https://nos.nl/artikel/126230-spanje-eerste-eu...  \n",
       "2  https://nos.nl/artikel/126233-fout-justitie-in...  \n",
       "3  https://nos.nl/artikel/126232-museumplein-vol-...  \n",
       "4  https://nos.nl/artikel/126236-obama-krijgt-rap...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('dutch-news-articles.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 220132 entries, 0 to 220131\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count   Dtype \n",
      "---  ------    --------------   ----- \n",
      " 0   datetime  220132 non-null  object\n",
      " 1   title     220132 non-null  object\n",
      " 2   content   220132 non-null  object\n",
      " 3   category  220132 non-null  object\n",
      " 4   url       220132 non-null  object\n",
      "dtypes: object(5)\n",
      "memory usage: 8.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unit of observation are articles. The dataset consists of **220132** unique articles/rows.\n",
    "### Cleaning the dataset\n",
    "Before I'm able to analyse the news articles, the raw data needs to be cleaned. \n",
    "The texts contain a lot of white spaces and some weird characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code part of this Notebook: https://www.kaggle.com/maxscheijen/text-mining-dutch-news-articles \n",
    "import unicodedata\n",
    "import re\n",
    "\n",
    "def clean_string(x):\n",
    "    x = re.sub(\" +\", \" \", x)\n",
    "    x = x.replace(\"\\n\", \" \")\n",
    "    x = unicodedata.normalize(\"NFKD\", x)\n",
    "    return x\n",
    "\n",
    "df['content'] = df['content'].progress_apply(clean_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "----\n",
    "\n",
    "The dataset consist of the following categories:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Buitenland          85582\n",
       "Binnenland          74433\n",
       "Politiek            20145\n",
       "Economie            17965\n",
       "Regionaal nieuws    13279\n",
       "Koningshuis          3012\n",
       "Opmerkelijk          2716\n",
       "Cultuur & Media      2046\n",
       "Tech                  954\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I want to use named entities to predict what category an article belongs to. A **named entity** is a real-world object, such as persons, locations, organizations, products, etc., that can be denoted with a proper name. [spaCy](https://spacy.io/) is an open-source software library helping with identifying named entities. There are several other models that can also extract named entities from a text, for example [NLTK](https://www.nltk.org/book/ch07.html). Iâ€™m using spaCy for this project because it has a Dutch model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Getting Named Entities with Spacy\n",
    "First I'm importing and loading spacy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import nl_core_news_sm\n",
    "nlp = nl_core_news_sm.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I explain with one article how spaCy works. Later I apply spaCy to every row of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.content.iloc[9]\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaCy returns different types of named entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Het tv-programma 'Ik \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    hou van Holland\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "' is op oudjaarsavond het best bekeken programma geweest. Er keken ruim ruim \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    twee\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " miljoen mensen naar het programma op RTL4. Goede \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    tweede\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " werd '\n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    De TV draait\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " door' met ruim anderhalf miljoen kijkers. Naar de oudejaarsconference van \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Jan Jaap van der Wal\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " keken \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1,3 miljoen mensen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ", net iets minder dan naar \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Paul de Leeuw\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", die daarna op dezelfde zender het jaar uitluidde met 'De \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Leeuw Knalt\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2010\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "'. Naar \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Guido Weijers\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       "' oudejaarsconference op \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    SBS\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    6\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " keken \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1,2 miljoen mensen\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(nlp(str(doc)), jupyter=True, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**While applying spaCy to the articles, I noticed the following:**\n",
    "\n",
    "1. when applying spaCy on all articles, it will generate many different and unique named entities.\n",
    "\n",
    "Therefore I decided to only use the 110 most common entities per category in a sparse matrix. This results in 9 * 110 = 990 named entities. \n",
    "\n",
    "2. Some named entities can mean something different in another contexts and some are therefore not usable. For example as seen in the example above: the words 'eerste' (means: first) and 'half jaar' (means: half a year).\n",
    "\n",
    "Therefore I decided to not use the following named entities types: `CARDINAL`, `ORDINAL`, `MONEY`, `DATE`, `QUANTITY`, `TIME`, `PERCENT`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I wrote the following code to get the most common 110 named entities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df['category'].unique()\n",
    "\n",
    "def export_named_entities(categories):\n",
    "    for category in categories:\n",
    "        df_category = df.loc[df['category'] == category]\n",
    "        # 1. get all named entities per category for every article\n",
    "        all_named_entities = df_category['content'].apply(\n",
    "            get_all_named_entities)\n",
    "       \n",
    "        # 2. calculate most common named entities for every category\n",
    "        sum_named_entities = count_named_entities(all_named_entities)\n",
    "        \n",
    "        # 3. export most common named entities per category to a .json file\n",
    "        create_json(category, sum_named_entities)\n",
    "\n",
    "# 1. get all named entities per category for every article\n",
    "def get_all_named_entities(row):\n",
    "    doc = nlp(row)\n",
    "    items = []\n",
    "    for entity in doc.ents:\n",
    "        if entity.label_ != 'CARDINAL' and entity.label_ != 'DATE' and entity.label_ != 'QUANTITY' and entity.label_ != 'TIME' and entity.label_ != 'ORDINAL' and entity.label_ != 'PERCENT' and entity.label_ != 'MONEY':\n",
    "            items.append(entity.text)\n",
    "    return items\n",
    "\n",
    "# 2. calculate most common named entities for every category\n",
    "def count_named_entities(all_named_entities):\n",
    "    sum_entities = []\n",
    "    for items in all_named_entities:\n",
    "        for item in items:\n",
    "            sum_entities.append(item)\n",
    "    sum_entities = dict(Counter(sum_entities).most_common(110)).keys() #return keys of 110 most common NE.\n",
    "    return sum_entities\n",
    "\n",
    "# 3. export most common named entities per category to a .json file\n",
    "def create_json(category, sum_named_entities):\n",
    "    print(category, sum_named_entities)\n",
    "    result = dict([(item, idx) for idx, item in enumerate(sum_named_entities)])\n",
    "    with open(\"./named-entities/\" + category + \".json\", \"w\") as out_file:\n",
    "        json.dump(result, out_file, ensure_ascii=False)\n",
    "\n",
    "        \n",
    "# I comment this out, because this takes a \n",
    "# long time and the results are stored in \n",
    "# json files.\n",
    "\n",
    "# export_named_entities(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results can be viewed in: https://github.com/sterrevangeest/dutch-news-articles/tree/master/named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 - Creating a Sparse Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the named entities, I needed to get the text into a form that a machine learning model and Python can understand and use to comprehend and train a model. The procedure which is used to convert the text into a form that Python and machine learning models can understand is called vectorising (Drikvandi & Lawal, 2020). To do this, I'm creating a sparse matrix.\n",
    "\n",
    "**I wrote the following code to create a sparse matrix for every named entity :**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json #because the named entities are saved in json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code part of this article: \n",
    "# https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067\n",
    "def mytokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "def create_sparse_matrix(df, vocab):\n",
    "    vectorizer = CountVectorizer(\n",
    "        vocabulary=vocab, tokenizer=mytokenizer, lowercase=False)\n",
    "\n",
    "    X = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "    count_vect_df = pd.DataFrame(\n",
    "        X.todense(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "    def convert_to_sparse_pandas(df, exclude_columns=[]):\n",
    "        \n",
    "        \"\"\"\n",
    "        Converts columns of a data frame into SparseArrays and returns the data frame with transformed columns.\n",
    "        Use exclude_columns to specify columns to be excluded from transformation.\n",
    "        :param df: pandas data frame\n",
    "        :param exclude_columns: list\n",
    "            Columns not be converted to sparse\n",
    "        :return: pandas data frame\n",
    "        \"\"\"\n",
    "        \n",
    "        df = df.copy()\n",
    "        exclude_columns = set(exclude_columns)\n",
    "\n",
    "        for (columnName, columnData) in df.iteritems():\n",
    "            if columnName in exclude_columns:\n",
    "                continue\n",
    "            df[columnName] = pd.arrays.SparseArray(\n",
    "                columnData.values, dtype='uint8')\n",
    "\n",
    "        return df\n",
    "\n",
    "    sparse_matrix_df = convert_to_sparse_pandas(count_vect_df)\n",
    "    return sparse_matrix_df\n",
    "\n",
    "\n",
    "# I comment this out, because this takes a \n",
    "# long time and the results are stored in \n",
    "# a dataframe (see below).\n",
    "\n",
    "# create a new column for every named entity in the json files\n",
    "\n",
    "#for category in categories:\n",
    "#    with open('./named-entities/' + category + \".json\") as json_file:\n",
    "#        vocab = json.load(json_file)\n",
    "        \n",
    "#        sparse_matrix = create_sparse_matrix(df, vocab)\n",
    "#        df = pd.concat([df, sparse_matrix], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to save memory I remove the cells \n",
    "# I don't need for the predicting. \n",
    "\n",
    "del df['content']\n",
    "del df['datetime']\n",
    "del df['title']\n",
    "\n",
    "\n",
    "# and save the \"new dataset\" to a new .csv file:\n",
    "\n",
    "# I comment this out, because this takes a \n",
    "# long time and the results are stored in\n",
    "# the dataframe.\n",
    "\n",
    "# df.to_csv('nos-sparse-matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse matrix is a matrix that consists of mostly zero's. For every time the content of the article contains the named entity, it adds a 1 to that named entity. This is what the data frame looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>url</th>\n",
       "      <th>Amerikaanse</th>\n",
       "      <th>Trump</th>\n",
       "      <th>VS</th>\n",
       "      <th>Britse</th>\n",
       "      <th>Rusland</th>\n",
       "      <th>Nederland</th>\n",
       "      <th>Russische</th>\n",
       "      <th>Franse</th>\n",
       "      <th>...</th>\n",
       "      <th>Gelderse</th>\n",
       "      <th>PVV.2</th>\n",
       "      <th>Zuid-Holland</th>\n",
       "      <th>Urk</th>\n",
       "      <th>Alkmaar</th>\n",
       "      <th>Roosendaal</th>\n",
       "      <th>Weert</th>\n",
       "      <th>Britse.7</th>\n",
       "      <th>Hengelo</th>\n",
       "      <th>A12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126231-enige-litouwse-k...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126230-spanje-eerste-eu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126233-fout-justitie-in...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Binnenland</td>\n",
       "      <td>https://nos.nl/artikel/126232-museumplein-vol-...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Buitenland</td>\n",
       "      <td>https://nos.nl/artikel/126236-obama-krijgt-rap...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 992 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                                url  Amerikaanse  \\\n",
       "0  Buitenland  https://nos.nl/artikel/126231-enige-litouwse-k...            0   \n",
       "1  Buitenland  https://nos.nl/artikel/126230-spanje-eerste-eu...            0   \n",
       "2  Buitenland  https://nos.nl/artikel/126233-fout-justitie-in...            2   \n",
       "3  Binnenland  https://nos.nl/artikel/126232-museumplein-vol-...            0   \n",
       "4  Buitenland  https://nos.nl/artikel/126236-obama-krijgt-rap...            1   \n",
       "\n",
       "   Trump  VS  Britse  Rusland  Nederland  Russische  Franse  ...  Gelderse  \\\n",
       "0      0   0       0        0          0          0       0  ...         0   \n",
       "1      0   0       1        0          0          0       0  ...         0   \n",
       "2      0   0       0        0          0          0       0  ...         0   \n",
       "3      0   0       0        0          0          0       0  ...         0   \n",
       "4      0   1       0        0          0          0       0  ...         0   \n",
       "\n",
       "   PVV.2  Zuid-Holland  Urk  Alkmaar  Roosendaal  Weert  Britse.7  Hengelo  \\\n",
       "0      0             0    0        0           0      0         0        0   \n",
       "1      0             0    0        0           0      0         1        0   \n",
       "2      0             0    0        0           0      0         0        0   \n",
       "3      0             0    0        0           0      0         0        0   \n",
       "4      0             0    0        0           0      0         0        0   \n",
       "\n",
       "   A12  \n",
       "0    0  \n",
       "1    0  \n",
       "2    0  \n",
       "3    0  \n",
       "4    0  \n",
       "\n",
       "[5 rows x 992 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sparse = pd.read_csv('nos-sparse-matrix.csv')\n",
    "\n",
    "del df_sparse['Unnamed: 0'] # deleting this extra index column, because I don't need this\n",
    "\n",
    "df_sparse.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I removed the categories**: \"culture & media\", \"remarkable\" and \"tech\" from the training and test data because they resulted in very bad predictions.\n",
    "\n",
    "I think this is because the named entities in these articles are all so unique that the named entities are not well represented enough in the sparse matrix. Because the articles in these categories probably also contain general entities: such as 'Europa' or 'Nederland', they are more likely to be estimated in other categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sparse = df_sparse.loc[df_sparse['category'] != 'Tech']\n",
    "df_sparse = df_sparse.loc[df_sparse['category'] != 'Opmerkelijk']\n",
    "df_sparse = df_sparse.loc[df_sparse['category'] != 'Cultuur & Media']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 - Creating a sparse matrix with Scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 1705.036 MB\n"
     ]
    }
   ],
   "source": [
    "BYTES_TO_MB_DIV = 0.000001\n",
    "def print_memory_usage_of_data_frame(df):\n",
    "    mem = round(df.memory_usage().sum() * BYTES_TO_MB_DIV, 3) \n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "    \n",
    "print_memory_usage_of_data_frame(df_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sparse data is a very big file. This is because sklearn does not handle sparsa dataframes as sparse data. Instead, sparse columns are converted to dense before being processed, causing the data frame size to explode (Velidou, 2019). \n",
    "\n",
    "**To be able to work with the sparse matrix I convertet the pandas data frame into a [scipy](https://www.scipy.org/) sparse matrix. I wrote the following code for that:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.preprocessing import normalize #get the function needed to normalize our data.\n",
    "\n",
    "# this code part of this article: \n",
    "# https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067\n",
    "def data_frame_to_scipy_sparse_matrix(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Converts a sparse pandas data frame to sparse scipy csr_matrix.\n",
    "    :param df: pandas data frame\n",
    "    :return: csr_matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    arr = lil_matrix(df.shape, dtype=np.float32)\n",
    "    for i, col in enumerate(df.columns):\n",
    "        ix = df[col] != 0\n",
    "        arr[np.where(ix), i] = 1\n",
    "\n",
    "    return arr.tocsr()\n",
    "\n",
    "\n",
    "y_sparse = df_sparse['category']\n",
    "X = df_sparse[df_sparse.columns.difference(['category', 'url'])]\n",
    "X_sparse = data_frame_to_scipy_sparse_matrix(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The new sparse matrix is significantly smaller than the original sparse dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage is 26.25266 MB\n"
     ]
    }
   ],
   "source": [
    "def get_csr_memory_usage(matrix):\n",
    "    mem = (X_sparse.data.nbytes + X_sparse.indptr.nbytes + X_sparse.indices.nbytes) * BYTES_TO_MB_DIV\n",
    "    print(\"Memory usage is \" + str(mem) + \" MB\")\n",
    "\n",
    "get_csr_memory_usage(X_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descriptive Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most found named entity is: Nederland and was found 56426 times in all the articles\n"
     ]
    }
   ],
   "source": [
    "sum_columns = []\n",
    "for (columnName, columnData) in X.iteritems():\n",
    "    sum_column = X[columnName].sum()\n",
    "    sum_columns.append([sum_column, columnName])\n",
    "\n",
    "max_sub = max(sum_columns, key=lambda x: x[0])\n",
    "max_sub\n",
    "\n",
    "print('The most found named entity is:', max_sub[1], 'and was found', max_sub[0], 'times in all the articles' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOS also talk a lot about itself :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOS was found 10956 times in all the articles\n"
     ]
    }
   ],
   "source": [
    "NOS = list(df_sparse['NOS'].value_counts())\n",
    "NOS.pop(0) #because the first elements counts al the 0 times NOS was found\n",
    "sum(NOS)\n",
    "print('NOS was found', sum(NOS), 'times in all the articles' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump was found 5184 times in all the articles\n"
     ]
    }
   ],
   "source": [
    "TRUMP = list(df_sparse['Trump'].value_counts())\n",
    "TRUMP.pop(0) #because the first elements counts al the 0 times TRUMP was found\n",
    "print('Trump was found', sum(TRUMP), 'times in all the articles' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training the model\n",
    "---\n",
    "\n",
    "I tried to fit different models on the data. First I split the data into training and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, y_sparse, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Logistic Regression: 0.7351263116984065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(random_state=0, multi_class='ovr', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy for Logistic Regression:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for SGDClassifier: 0.7321103769918383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy for SGDClassifier:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for KNeighborsClassifier: 0.7321103769918383\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "neigh.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy for KNeighborsClassifier:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor\n",
    "\n",
    "This one took VERY long, so I don't have an result for this algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "categories = y_train.unique()\n",
    "le.fit(categories)\n",
    "list(le.classes_)\n",
    "y_train = le.transform(y_train)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the different algorithms are very close to each other. The Logistic Regression algorithm has the highest accuracy for now. I will continue with this algorithm because this algorithm is quite fast in comparison to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy for Logistic Regression: 0.7351263116984065\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(random_state=0, multi_class='ovr', solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"The accuracy for Logistic Regression:\", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This means the model is **73,5%** accurate. I think this is not very bad. But it still means that 30% of the articles are not predicted correctly. \n",
    "\n",
    "While applying the named entities, I encountered some duplicates in the named entities. By removing these errors and increasing the number of named entities, I think the model can perform better. Also I think the model can be improved by looking at named entities that are correlated with eachother. For example: 'Nederland' and 'Nederlandse'.  \n",
    "\n",
    "To furter evaluate the model I created a classification report and a confusion matrix.\n",
    "\n",
    "**Clasification report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "      Binnenland       0.71      0.70      0.71     22467\n",
      "      Buitenland       0.75      0.92      0.83     25486\n",
      "        Economie       0.68      0.39      0.49      5353\n",
      "     Koningshuis       0.65      0.40      0.50       865\n",
      "        Politiek       0.75      0.60      0.67      6091\n",
      "Regionaal nieuws       0.82      0.47      0.60      4063\n",
      "\n",
      "        accuracy                           0.74     64325\n",
      "       macro avg       0.73      0.58      0.63     64325\n",
      "    weighted avg       0.73      0.74      0.72     64325\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Precision** is the ratio of correctly predicted positive observations to the total predicted positive observations. The question that this metric answers is: *Of all categories that are labeled as X, how many actually are X?* A high precision relates to a low false positive rate. The model gives 0.73 precision. In this context the precision is not bad. But for actually implementing a model like this the presicion should be higher. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recall** is the ratio of correctly predicted positive observations to the all observations in actual class. The question recall answers is: *Of all the categories that truly are X, how many did we label?* The model got a quite high recall for the category 'Buitenland', also 'Binnenland' is not doing too bad. But the model is actually quite bad for prediciting the 'Economie' and 'Koningshuis' categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PRED: Domestic</th>\n",
       "      <th>PRED: Foreign</th>\n",
       "      <th>PRED: Economy</th>\n",
       "      <th>PRED: Royal Family</th>\n",
       "      <th>PRED: Politics</th>\n",
       "      <th>PRED: Regional News</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Domestic</th>\n",
       "      <td>15771</td>\n",
       "      <td>4793</td>\n",
       "      <td>509</td>\n",
       "      <td>109</td>\n",
       "      <td>898</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Foreign</th>\n",
       "      <td>1507</td>\n",
       "      <td>23510</td>\n",
       "      <td>265</td>\n",
       "      <td>46</td>\n",
       "      <td>150</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Economy</th>\n",
       "      <td>1690</td>\n",
       "      <td>1440</td>\n",
       "      <td>2075</td>\n",
       "      <td>5</td>\n",
       "      <td>134</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Royal Family</th>\n",
       "      <td>236</td>\n",
       "      <td>253</td>\n",
       "      <td>7</td>\n",
       "      <td>346</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Politics</th>\n",
       "      <td>1586</td>\n",
       "      <td>651</td>\n",
       "      <td>159</td>\n",
       "      <td>23</td>\n",
       "      <td>3663</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Regional News</th>\n",
       "      <td>1482</td>\n",
       "      <td>612</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PRED: Domestic  PRED: Foreign  PRED: Economy  \\\n",
       "Domestic                15771           4793            509   \n",
       "Foreign                  1507          23510            265   \n",
       "Economy                  1690           1440           2075   \n",
       "Royal Family              236            253              7   \n",
       "Politics                 1586            651            159   \n",
       "Regional News            1482            612             24   \n",
       "\n",
       "               PRED: Royal Family  PRED: Politics  PRED: Regional News  \n",
       "Domestic                      109             898                  387  \n",
       "Foreign                        46             150                    8  \n",
       "Economy                         5             134                    9  \n",
       "Royal Family                  346              20                    3  \n",
       "Politics                       23            3663                    9  \n",
       "Regional News                   1              22                 1922  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "conf_matrix = pd.DataFrame(cnf_matrix, index=[\"Domestic\", \"Foreign\", \"Economy\", \"Royal Family\", \"Politics\", \"Regional News\" ], \n",
    "                                   columns = [\"PRED: Domestic\", \"PRED: Foreign\", \"PRED: Economy\", \"PRED: Royal Family\", \"PRED: Politics\", \"PRED: Regional News\" ]) \n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The confusion matrix shows the summary of the prediction results. \n",
    "\n",
    "This matrix, and the other results, show that it is quite difficult to predict a category based on named entities. I personally think this is because some named entities are very general and therefore appear in different categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "____\n",
    "\n",
    "- Drikvandi, R., & Lawal, O. (2020). Sparse Principal Component Analysis for Natural Language Processing. Annals of Data Science, 0. https://doi.org/10.1007/s40745-020-00277-x\n",
    "- Velidou, D. S. (2019, November 5). Working with sparse data sets in pandas and sklearn. Medium. https://towardsdatascience.com/working-with-sparse-data-sets-in-pandas-and-sklearn-d26c1cfbe067\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
